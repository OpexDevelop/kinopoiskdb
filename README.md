# Автоматический парсер Кинопоиска для Hugging Face

Этот проект использует GitHub Actions для автоматического сбора данных с API Кинопоиска и их сохранения в датасет на Hugging Face. Система полностью автоматизирована и спроектирована для долгосрочной, отказоустойчивой работы.

## Архитектура

Процесс разделен на два независимых, но связанных воркфлоу для максимальной надежности:

Ежедневный запуск (по расписанию) ->  
1. Daily Collector -> Сохранение "сырой" части в /raw_data -> Запуск консолидатора ->  
2. Consolidator -> Создание чистого файла в /consolidated

## Ключевые особенности

* Полная автоматизация: Сбор и обработка данных происходят без вашего участия
* Отказоустойчивость: Встроенные механизмы повторных попыток для борьбы с временными сбоями API
* "Вечный" цикл обновлений: После прохождения всех страниц парсер автоматически начинает новый круг для обновления уже существующих данных
* Умная дедупликация: Консолидатор всегда оставляет только самую свежую версию фильма, основываясь на дате `updatedAt`
* Подробное логирование: Каждый запуск генерирует детальный лог-файл, который сохраняется как артефакт

## Как это работает

### 1. Daily Collector (Ежедневный сборщик)
* Запускается по расписанию (раз в день) или вручную
* Определяет, на какой странице остановился в прошлый раз, читая служебный файл `_metadata.json`
* Скачивает новую порцию данных (200 запросов по умолчанию)
* Если доходит до конца, автоматически начинает новый круг с первой страницы для сбора обновлений
* Сохраняет "сырую" порцию данных в уникальный архив в папку `/raw_data` в датасете
* После успешного завершения автоматически запускает второй воркфлоу

### 2. Consolidator (Консолидатор)
* Запускается автоматически после успешного завершения сборщика (или вручную)
* Скачивает все "сырые" архивы из папки `/raw_data`
* Обрабатывает их в памяти, удаляя дубликаты и оставляя только самую свежую версию каждого фильма
* Сохраняет один итоговый, чистый файл `consolidated_data.jsonl.gz` в папку `/consolidated`

## Первоначальная настройка
* Создайте репозиторий на GitHub и поместите в него все файлы проекта
* Настройте секреты в GitHub. Перейдите в вашем репозитории в **Settings > Secrets and variables > Actions** и нажмите **New repository secret**. Создайте два секрета:
  * `KINOPOISK_API_KEY`: Ваш ключ от API Кинопоиска
  * `HF_TOKEN`: Ваш токен от Hugging Face. Убедитесь, что у него есть права на запись (write)
* Запустите воркфлоу в первый раз. Перейдите на вкладку **Actions**, выберите **1. Kinopoisk Daily Collector** и запустите его вручную. Он автоматически создаст датасет на Hugging Face, если тот не существует

## Использование
* Автоматический режим: Система будет работать сама по расписанию, указанному в `daily_collector.yml`
* Ручной запуск: Вы можете в любой момент запустить любой из воркфлоу через вкладку **Actions** на GitHub

## Конфигурация
* Лимит запросов за запуск: Чтобы изменить количество запросов, которые делает ежедневный сборщик, откройте файл `daily_collector.py` и измените значение переменной `MAX_REQUESTS_PER_RUN`

## Структура датасета на Hugging Face
* `/raw_data/`: Папка с небольшими "сырыми" архивами данных, по одному на каждый запуск сборщика. Это ваш полный, неизменяемый лог
* `/consolidated/`: Папка с одним большим, очищенным и готовым к использованию файлом `consolidated_data.jsonl.gz`
* `/_metadata.json`: Служебный файл в корне, который хранит номер последней обработанной страницы
